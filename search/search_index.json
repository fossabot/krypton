{"config":{"lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"Krypton \u00b6 Model Server for ML and DL Models built using FastAPI Deploy Machine Learning and Deep Learning models using simple Python scripts Note This project is in development stage. You can expect rapid changes in the internals of the project. Would love to hear valuable feedback, and it will be duly acknowledged. Any one who has experienced an issue / want to report a bug can create a issue here in Github Work in progress \u00b6 Unit and Integration Test Cases Authentication for API's (Basic Auth, API Key based auth) Metrics using statsd exporter Configurable logging options License \u00b6 Krypton ML uses Apache License 2.0. Find the license here About \u00b6 Author: Varun Kruthiventi Email: varunk@ieee.org","title":"Krypton"},{"location":"#krypton","text":"Model Server for ML and DL Models built using FastAPI Deploy Machine Learning and Deep Learning models using simple Python scripts Note This project is in development stage. You can expect rapid changes in the internals of the project. Would love to hear valuable feedback, and it will be duly acknowledged. Any one who has experienced an issue / want to report a bug can create a issue here in Github","title":"Krypton"},{"location":"#work-in-progress","text":"Unit and Integration Test Cases Authentication for API's (Basic Auth, API Key based auth) Metrics using statsd exporter Configurable logging options","title":"Work in progress"},{"location":"#license","text":"Krypton ML uses Apache License 2.0. Find the license here","title":"License"},{"location":"#about","text":"Author: Varun Kruthiventi Email: varunk@ieee.org","title":"About"},{"location":"accesing-models/","text":"Accesing Models \u00b6 Get List of Models \u00b6 Once the krypton server is up and running, you can access the list of models by making a GET request on this endpoint. Request \u00b6 curl --location --request GET 'http://<hostname>:<port>/api/v1/models' Sample Response \u00b6 { \"success\" : true , \"data\" : [ { \"model\" : \"spacy_ner_demo\" , \"endpoint\" : \"/api/v1/models/spacy_ner_demo/predict\" , \"status\" : \"Available\" } ] }","title":"Accesing Models"},{"location":"accesing-models/#accesing-models","text":"","title":"Accesing Models"},{"location":"accesing-models/#get-list-of-models","text":"Once the krypton server is up and running, you can access the list of models by making a GET request on this endpoint.","title":"Get List of Models"},{"location":"accesing-models/#request","text":"curl --location --request GET 'http://<hostname>:<port>/api/v1/models'","title":"Request"},{"location":"accesing-models/#sample-response","text":"{ \"success\" : true , \"data\" : [ { \"model\" : \"spacy_ner_demo\" , \"endpoint\" : \"/api/v1/models/spacy_ner_demo/predict\" , \"status\" : \"Available\" } ] }","title":"Sample Response"},{"location":"adding-models/","text":"Adding Models \u00b6 Once the models directory is created by the cli, you can start adding models to krypton by creating model files in the directory by using the Krypton's KryptonModel class. You can use this example, which has a Spacy based implementation of Krypton Model script. Spacy Example \u00b6 import spacy from krypton import KryptonModel from fastapi.responses import JSONResponse class SpacyDemo ( KryptonModel ): model_name = 'spacy_ner_demo' model = None def load_model ( self ): self . model = spacy . load ( \"en_core_web_sm\" ) async def predict ( self , request ): request_json = await request . json () data = request_json . get ( 'data' ) doc = self . model ( data ) response = { 'noun_phrases' : [ chunk . text for chunk in doc . noun_chunks ], 'verbs' : [ token . lemma_ for token in doc if token . pos_ == \"VERB\" ], 'entities' : [{ 'ents' : entity . text , 'label' : entity . label_ } for entity in doc . ents ] } return JSONResponse ( status_code = 200 , content = response ) model = SpacyDemo () The sample used in this model example is taken from Spacy.io Implementation \u00b6 Model Class \u00b6 Krypton server expects every model script to have a class implemented based on KryptonModel base class. It can be imported from krypton root package from krypton import KryptonModel The class should implement load_model and predict methods - this is mandatory. The class should have the attributes model_name and model - this is mandatory. predict method is expected to be an async function to support FastAPI's request object - this is mandatory. load_model method is called during the startup of Krypton model server. The server will try to call this method to load the model into memory and make it available for API requests. model_name attribute is used by the server predict method is called during the the execution of API requests for the specific model. A single parameter, request which is of type Request from fastapi module is injected into the method during API calls. This request object contains the request parameters like body, parsed form-data (can be used for file uploads), json body and even headers of the request. Please refer Starlette's documentation for details about Request class. The developer can carryout the necessary computations for making the predictions using model attribute and then return a valid response. This response has to be a valid response object that can be handled by FastAPI. model callable \u00b6 Krypton server expects every model script to have a object with name model which needs to be instantiated with any class, that implements KryptonModel . Without the model callable, the Krypton server would throw expcetion while booting. Model dependencies \u00b6 The developer needs to make sure that the model specific dependencies are added to the Python environment where krypton module was installed. It is always recommended to use a new virtualenv for using Krypton. Apply Changes \u00b6 Once you have added a model script, you can restart the server by using the krypton server command again. After restarting the krypton server, all the model scripts present in the ~/krypton/models will be loaded into server. Check the next page on how to get list of models available and access the model endpoints.","title":"Adding Models"},{"location":"adding-models/#adding-models","text":"Once the models directory is created by the cli, you can start adding models to krypton by creating model files in the directory by using the Krypton's KryptonModel class. You can use this example, which has a Spacy based implementation of Krypton Model script.","title":"Adding Models"},{"location":"adding-models/#spacy-example","text":"import spacy from krypton import KryptonModel from fastapi.responses import JSONResponse class SpacyDemo ( KryptonModel ): model_name = 'spacy_ner_demo' model = None def load_model ( self ): self . model = spacy . load ( \"en_core_web_sm\" ) async def predict ( self , request ): request_json = await request . json () data = request_json . get ( 'data' ) doc = self . model ( data ) response = { 'noun_phrases' : [ chunk . text for chunk in doc . noun_chunks ], 'verbs' : [ token . lemma_ for token in doc if token . pos_ == \"VERB\" ], 'entities' : [{ 'ents' : entity . text , 'label' : entity . label_ } for entity in doc . ents ] } return JSONResponse ( status_code = 200 , content = response ) model = SpacyDemo () The sample used in this model example is taken from Spacy.io","title":"Spacy Example"},{"location":"adding-models/#implementation","text":"","title":"Implementation"},{"location":"adding-models/#model-class","text":"Krypton server expects every model script to have a class implemented based on KryptonModel base class. It can be imported from krypton root package from krypton import KryptonModel The class should implement load_model and predict methods - this is mandatory. The class should have the attributes model_name and model - this is mandatory. predict method is expected to be an async function to support FastAPI's request object - this is mandatory. load_model method is called during the startup of Krypton model server. The server will try to call this method to load the model into memory and make it available for API requests. model_name attribute is used by the server predict method is called during the the execution of API requests for the specific model. A single parameter, request which is of type Request from fastapi module is injected into the method during API calls. This request object contains the request parameters like body, parsed form-data (can be used for file uploads), json body and even headers of the request. Please refer Starlette's documentation for details about Request class. The developer can carryout the necessary computations for making the predictions using model attribute and then return a valid response. This response has to be a valid response object that can be handled by FastAPI.","title":"Model Class"},{"location":"adding-models/#model-callable","text":"Krypton server expects every model script to have a object with name model which needs to be instantiated with any class, that implements KryptonModel . Without the model callable, the Krypton server would throw expcetion while booting.","title":"model callable"},{"location":"adding-models/#model-dependencies","text":"The developer needs to make sure that the model specific dependencies are added to the Python environment where krypton module was installed. It is always recommended to use a new virtualenv for using Krypton.","title":"Model dependencies"},{"location":"adding-models/#apply-changes","text":"Once you have added a model script, you can restart the server by using the krypton server command again. After restarting the krypton server, all the model scripts present in the ~/krypton/models will be loaded into server. Check the next page on how to get list of models available and access the model endpoints.","title":"Apply Changes"},{"location":"installation/","text":"Installation \u00b6 Using pip \u00b6 The krypton module can be installed from the PyPI repository using pip install krypton-ml It is always recommended to use a new virtualenv for using Krypton. Starting Server \u00b6 Once the package is installed, you can start the Krypton Model server using Krypton CLI krypton server For the first time, the krypton model server would try to create directory at ~/krypton/models by default. This will vary depending upon the kind of os you are using. The location can look like these for each operating system: Mac: /Users/<user_name>/krypton/models Linux: /home/<user_name>/krypton/models Windows: C:\\Users\\<user_name>\\krypton\\models This path can be modified to a custom location by setting KRYPTON_APP_ROOT value to any valid location where you want krypton server to setup the models directory. The server would be started at PORT 7000 by default, and it can be accessed at http://localhost:7000","title":"Installation"},{"location":"installation/#installation","text":"","title":"Installation"},{"location":"installation/#using-pip","text":"The krypton module can be installed from the PyPI repository using pip install krypton-ml It is always recommended to use a new virtualenv for using Krypton.","title":"Using pip"},{"location":"installation/#starting-server","text":"Once the package is installed, you can start the Krypton Model server using Krypton CLI krypton server For the first time, the krypton model server would try to create directory at ~/krypton/models by default. This will vary depending upon the kind of os you are using. The location can look like these for each operating system: Mac: /Users/<user_name>/krypton/models Linux: /home/<user_name>/krypton/models Windows: C:\\Users\\<user_name>\\krypton\\models This path can be modified to a custom location by setting KRYPTON_APP_ROOT value to any valid location where you want krypton server to setup the models directory. The server would be started at PORT 7000 by default, and it can be accessed at http://localhost:7000","title":"Starting Server"},{"location":"making-predictions/","text":"Make Prediction on a model \u00b6 Once the krypton server is up and running, you can access the list of models by making a GET request on this endpoint. Request \u00b6 curl --location --request POST 'http://<hostname>:<port>/api/v1/models/<model_name>/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\" }' Sample Response \u00b6 { \"noun_phrases\" : [ \"Sebastian Thrun\" , \"self-driving cars\" , \"Google\" , \"few people\" , \"the company\" , \"him\" ], \"verbs\" : [ \"start\" , \"work\" , \"drive\" , \"take\" ], \"entities\" : [ { \"ents\" : \"Sebastian Thrun\" , \"label\" : \"PERSON\" }, { \"ents\" : \"Google\" , \"label\" : \"ORG\" }, { \"ents\" : \"2007\" , \"label\" : \"DATE\" } ] } The sample used in this model example is taken from Spacy.io","title":"Making predictions"},{"location":"making-predictions/#make-prediction-on-a-model","text":"Once the krypton server is up and running, you can access the list of models by making a GET request on this endpoint.","title":"Make Prediction on a model"},{"location":"making-predictions/#request","text":"curl --location --request POST 'http://<hostname>:<port>/api/v1/models/<model_name>/predict' \\ --header 'Content-Type: application/json' \\ --data-raw '{ \"data\": \"When Sebastian Thrun started working on self-driving cars at Google in 2007, few people outside of the company took him seriously.\" }'","title":"Request"},{"location":"making-predictions/#sample-response","text":"{ \"noun_phrases\" : [ \"Sebastian Thrun\" , \"self-driving cars\" , \"Google\" , \"few people\" , \"the company\" , \"him\" ], \"verbs\" : [ \"start\" , \"work\" , \"drive\" , \"take\" ], \"entities\" : [ { \"ents\" : \"Sebastian Thrun\" , \"label\" : \"PERSON\" }, { \"ents\" : \"Google\" , \"label\" : \"ORG\" }, { \"ents\" : \"2007\" , \"label\" : \"DATE\" } ] } The sample used in this model example is taken from Spacy.io","title":"Sample Response"}]}